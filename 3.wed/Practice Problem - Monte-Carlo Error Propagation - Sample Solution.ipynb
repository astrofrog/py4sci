{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Problem - Monte-Carlo Error Propagation - Sample Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start off by defining the values and uncertainties, along with some constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defined constant(s)\n",
    "G = 6.67384e-11  # SI units\n",
    "\n",
    "# Define values to sample from\n",
    "mean_m_1 = 40.e4\n",
    "sigma_m_1 = 0.05e4\n",
    "mean_m_2 = 30.e4\n",
    "sigma_m_2 = 0.1e4\n",
    "mean_r = 3.2\n",
    "sigma_r = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the mean and uncertainty in the force using standard error propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute mean and error of force\n",
    "mean_f = G * mean_m_1 * mean_m_2 / mean_r ** 2\n",
    "sigma_f = mean_f * np.sqrt((sigma_m_1 / mean_m_2) ** 2\n",
    "                           + (sigma_m_2 / mean_m_2) ** 2\n",
    "                           + 4. * (sigma_r / mean_r) ** 2)\n",
    "print(mean_f, sigma_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute this using Monte-Carlo error propagation. We sample ``N`` initial values that are drawn from the initial distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 1000000\n",
    "m_1 = np.random.normal(mean_m_1, sigma_m_1, N)\n",
    "m_2 = np.random.normal(mean_m_2, sigma_m_2, N)\n",
    "r = np.random.normal(mean_r, sigma_r, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for each sample, we can compute the final value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F = G * m_1 * m_2 / r ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print for these the mean and standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.mean(F), np.std(F))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is similar to the values found above, but in fact we have the full distribution of values, which we can plot a histogram for, along with a curve showing the Gaussian function for the result found from standard error propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define range of output values for plotting\n",
    "xmin = 0.75\n",
    "xmax = 0.82\n",
    "\n",
    "# Define Gaussian function\n",
    "\n",
    "def gaussian(x, mu, sigma):\n",
    "    norm = 1. / (sigma * np.sqrt(2. * np.pi))\n",
    "    return norm * np.exp(-(x - mu) ** 2. / (2. * sigma ** 2))\n",
    "\n",
    "x = np.linspace(xmin, xmax, 1000)\n",
    "y = gaussian(x, mean_f, sigma_f)\n",
    "\n",
    "plt.hist(F, bins=50, range=[xmin, xmax], normed=True)\n",
    "plt.plot(x, y, color='red', lw=3)\n",
    "plt.xlabel(\"Force (N)\")\n",
    "plt.ylabel(\"Relative Probability\")\n",
    "plt.xlim(xmin, xmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two agree very nicely. Now let's repeat this, but with larger initial errors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the sampled values agrees well with that found from standard error propagation. We can now repeat the experiment with larger uncertainties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define values to sample from\n",
    "mean_m_1 = 40.e4\n",
    "sigma_m_1 = 2.e4\n",
    "mean_m_2 = 30.e4\n",
    "sigma_m_2 = 10.e4\n",
    "mean_r = 3.2\n",
    "sigma_r = 1.0\n",
    "\n",
    "# Define range of output values for plotting\n",
    "xmin = -1.\n",
    "xmax = 5.\n",
    "\n",
    "# Define number of samples\n",
    "N = 1000000\n",
    "\n",
    "## STANDARD ERROR PROPAGATION\n",
    "\n",
    "# Compute mean and error of force\n",
    "mean_f = G * mean_m_1 * mean_m_2 / mean_r ** 2\n",
    "sigma_f = mean_f * np.sqrt((sigma_m_1 / mean_m_2) ** 2\n",
    "                           + (sigma_m_2 / mean_m_2) ** 2\n",
    "                           + 4. * (sigma_r / mean_r) ** 2)\n",
    "\n",
    "# Define Gaussian function\n",
    "x = np.linspace(xmin, xmax, 1000)\n",
    "y = gaussian(x, mean_f, sigma_f)\n",
    "\n",
    "## MONTE-CARLO ERROR PROPAGATION\n",
    "\n",
    "# Sample from initial values\n",
    "m_1 = np.random.normal(mean_m_1, sigma_m_1, N)\n",
    "m_2 = np.random.normal(mean_m_2, sigma_m_2, N)\n",
    "r = np.random.normal(mean_r, sigma_r, N)\n",
    "\n",
    "# Compute final values\n",
    "F = G * m_1 * m_2 / r ** 2\n",
    "\n",
    "## PLOTTING\n",
    "\n",
    "plt.hist(F, bins=50, range=[xmin, xmax], normed=True)\n",
    "plt.plot(x, y, color='red', lw=3)\n",
    "plt.xlabel(\"Force (N)\")\n",
    "plt.ylabel(\"Relative Probability\")\n",
    "plt.xlim(xmin, xmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of sampled points is now significantly non-Gaussian, which is normal because the uncertainties are large, and standard error propagation only works for small errors. The conclusion is that Monte-Carlo propagation is easier to code (because one doesn't need to remember all the propagation equations) and is also more correct because it can take into account non-Gaussian distributions (of input or output)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
